{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02.03.2020\n",
    "###  Работа с данными \n",
    "## Семинар 1\n",
    "\n",
    "Навигация:\n",
    " - [Задача про трафик](#Задача-1)\n",
    " - [Титаник](#Задача-2.-Титаник)\n",
    " - [Байесовский классификатор](#Задача-3.-Байесовский-классификатор-текстов)\n",
    "      - [Реализация](#Нормальная-реализация-классификатора)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1\n",
    "\n",
    "2/3 трафика идет на прямые ссылки из поисковых систем, конверсия в просмотр 10%. Остальной трафик идет на ЦПБ, просмотров из него 30%. Найти общую конверию в просмотры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение:\n",
    "\n",
    "Событие $А$ - просмотр,\n",
    "$H_1$ - прямой переход, $P(H_1) = 2/3$, $P(A|H_1) = 0.1$\n",
    "\n",
    "$H_2$ - переход из ЦПБ, $P(H_2) = 1/3$, $P(A|H_2) = 0.3$\n",
    "\n",
    "По формуле полной вероятности $P(A)= P(H_1)P(A|H_1)+P(H_2)P(A|H_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конверсия по обоим каналам равна 16.67%\n"
     ]
    }
   ],
   "source": [
    "pA = 2/3*0.1+1/3*0.3\n",
    "print(f\"Конверсия по обоим каналам равна {pA*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2. Титаник\n",
    "\n",
    "По данным колонок Class и Survived из датасета пассажиров Титаника найти вероятность выжить для пассажира 1 класса\n",
    "\n",
    "$$\n",
    "P(Survived = Yes | Class= 1st) = \\frac{P(Class=1 | Survived = Yes) \\cdot P(Survived = Yes)}{P(Class=1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение:\n",
    "\n",
    "В задачах с реальными данными вероятности заменяются частотами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1st</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1st</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1st</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1st</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1st</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class Survived\n",
       "0   1st      yes\n",
       "1   1st      yes\n",
       "2   1st      yes\n",
       "3   1st      yes\n",
       "4   1st      yes"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#загрузка датасета\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Dataset.data', sep = ',', header=None, \n",
    "                 names=['Class','Age','Gender','Survived'], usecols = ['Class', 'Survived'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность выжить, если пассажир путешествовал первым классом 62.46 %\n",
      "Априорная вероятность выжить  32.30 %\n"
     ]
    }
   ],
   "source": [
    "total_num = df.count()[0] #2201\n",
    "pS = df[df.Survived == 'yes'].count()[0]/total_num #711\n",
    "\n",
    "p1 = df[df.Class == '1st'].count()[0]/total_num #325\n",
    "\n",
    "p1S = df[(df.Class == '1st') & (df.Survived=='yes')].count()[0]/df[df.Survived == 'yes'].count()[0] #203/711\n",
    "\n",
    "pS1 = p1S*pS/p1\n",
    "print(f\"Вероятность выжить, если пассажир путешествовал первым классом {pS1*100:.2f} %\")\n",
    "print(f\"Априорная вероятность выжить  {pS*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность выжить, если пассажир путешествовал первым классом 62.46 %\n"
     ]
    }
   ],
   "source": [
    "# а вообще можно было сделать сразу так\n",
    "\n",
    "pS1a = df[(df.Class == '1st') & (df.Survived=='yes')].count()[0]/df[df.Class == '1st'].count()[0]\n",
    "print(f\"Вероятность выжить, если пассажир путешествовал первым классом {pS1a*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3. Байесовский классификатор текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью теоремы Байеса определить по тексту сообщения его класс (spam/ham)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обобщение предыдущей задачи на многомерный случай:**\n",
    "\n",
    "$$\n",
    "c_{MAP} = \\arg \\max_{\\substack{c \\in C}}P(c \\mid X) = \\arg \\max_{\\substack{c \\in C}}\\frac{P(c)P(X\\mid c)}{P(X)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_{MAP} = \\arg \\max_{\\substack{c \\in C}}P(c)P(x_1\\ldots x_n\\mid c)\n",
    "$$\n",
    "\n",
    "$P(c)$ вычисляем как # сообщений класса/общее # сообщений.\n",
    "\n",
    "$P(x_1\\ldots x_n\\mid c)$ в предположении о независимочти $x_i$ вычисляется как\n",
    "$$\n",
    "P(x_1\\ldots x_n\\mid c) = P(x_1 \\mid c)\\cdot \\ldots \\cdot P(x_n\\mid c) = \\prod_{i=1}^nP(x_i\\mid c)\n",
    "$$\n",
    "\n",
    "Обучение наивного байесовского классификатора сводится к вычислению по корпусу текстов (тренировочных данных) относительных частот по категориям, тогда мы получаем т.н *multinomial bayes model*:\n",
    "$$\n",
    "\\forall i,j: P(x_i \\mid c_j) = \\frac{n_{c_j}(x_i)}{\\sum_{k\\in V}n_{c_k}(x_i)}\n",
    "$$\n",
    "где $n_{c_j}(x_i)$ - количество раз, которое слово $x_i$ встречается в теме $c_j$, а $V$ - *словарь* корпуса документов, множество всех уникальных слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тупой способ: без оформления в функцию**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\79850\\\\Documents\\\\MAI-data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                                sms\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = '..\\\\SMSSpamCollection'\n",
    "df = pd.read_csv(\n",
    "    filename,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['class','sms'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects, num_features = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPAM_CLASS = 'spam'\n",
    "HAM_CLASS = 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sms_num = (df['class'] == SPAM_CLASS).sum()\n",
    "ham_sms_num = (df['class'] == HAM_CLASS).sum()\n",
    "\n",
    "\n",
    "# априорные вероятности \n",
    "p_spam = spam_sms_num / num_objects\n",
    "p_ham = ham_sms_num / num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1341, 0.8659\n"
     ]
    }
   ],
   "source": [
    "print(f'{p_spam:.4f}, {p_ham:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = 'Free'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'freemsg hey there darling its been 3 weeks now and no word back id like some fun you up for it still tb ok xxx std chgs to send £150 to rcv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# предварительная обработка строки\n",
    "import string\n",
    "\n",
    "sms_example = df['sms'].values[5] # одна строка для примера\n",
    "sms_example = ''.join([char for char in sms_example if char not in string.punctuation]) # удаляем знаки препинания\n",
    "sms_example = ' '.join([word.lower() for word in sms_example.split()]) # приводим слова к нижнему регистру\n",
    "sms_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оформим это как функцию\n",
    "\n",
    "def text_preprocess(sms_text: str):\n",
    "    \"\"\"Преобразоавние текста для анализа\"\"\"\n",
    "    text_no_punctuation = ''.join([char for char in sms_text if char not in string.punctuation])\n",
    "    text_lowercase = ' '.join([word.lower() for word in text_no_punctuation.split()])\n",
    "     \n",
    "    return text_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sms</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                                sms  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.assign(processed_text = df['sms'].apply(text_preprocess)) # добавляем колонку с обработанным текстом\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вероятность встретить тестовое слово в спаме\n",
    "\n",
    "spam_test_word_entries = df[df['class']==SPAM_CLASS]['processed_text'].apply(lambda row: test_word in row).sum()\n",
    "# вероятность встретить слово в не-спам смс\n",
    "ham_test_word_entries = df[df['class'] == HAM_CLASS]['processed_text'].apply(lambda row: test_word in row).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(word=\"free\"|class=spam)=0.2664\n",
      "P(word=\"free\"|class=not_spam)=0.0137\n"
     ]
    }
   ],
   "source": [
    "print(f'P(word=\"{test_word}\"|class=spam)={spam_test_word_entries/spam_sms_num:.4f}')\n",
    "print(f'P(word=\"{test_word}\"|class=not_spam)={ham_test_word_entries/ham_sms_num:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** слово \"free\"  - хороший маркер спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение наивного байесовского классификатора сводится к вычислению по корпусу текстов (тренировочных данных) относительных частот по категориям, тогда мы получаем т.н *multinomial bayes model*:\n",
    "$$\n",
    "\\forall i,j: P(x_i \\mid c_j) = \\frac{n_{c_j}(x_i)}{\\sum_{k\\in V}n_{c_k}(x_i)}\n",
    "$$\n",
    "где $n_{c_j}(x_i)$ - количество раз, которое слово $x_i$ встречается в теме $c_j$, а $V$ - *словарь* корпуса документов, множество всех уникальных слов\n",
    "\n",
    "\n",
    "$$\n",
    "c_{MAP} = \\arg \\max_{\\substack{c \\in C}}P(c)P(x_1\\ldots x_n\\mid c)\n",
    "$$\n",
    "\n",
    "$P(c)$ вычисляем как # сообщений класса/общее # сообщений.\n",
    "\n",
    "$P(x_1\\ldots x_n\\mid c)$ в предположении о независимочти $x_i$ вычисляется как\n",
    "$$\n",
    "P(x_1\\ldots x_n\\mid c) = P(x_1 \\mid c)\\cdot \\ldots \\cdot P(x_n\\mid c) = \\prod_{i=1}^nP(x_i\\mid c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормальная реализация классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \"\"\"Атрибуты\"\"\"\n",
    "    #TODO - какие нужны атрибуты\n",
    "    \n",
    "    SPAM_CLASS\n",
    "    HAM_CLASS\n",
    "    \n",
    "    #spam_num\n",
    "    #ham_num\n",
    "    \n",
    "    \n",
    "    def __init__(self, spam='spam', ham='ham'):\n",
    "        # TODO атрибуты по умолчанию\n",
    "        self.SPAM_CLASS = spam\n",
    "        self.HAM_CLASS = ham         \n",
    "\n",
    "    \n",
    "    def sms_preprocess(sms_text: str):\n",
    "        \"\"\"Преобразоавние текста смс для анализа\n",
    "        \n",
    "        :param sms_text: текст отдельного смс\n",
    "        \"\"\"\n",
    "        text_no_punctuation = ''.join([char for char in sms_text if char not in string.punctuation])\n",
    "        word_list_lowecase = [word.lower() for word in text_no_punctuation.split()]\n",
    "     \n",
    "        return word_list_lowercase\n",
    "    \n",
    "    def words_num(processed_data: list, target: list, word: str, sms_class: str): \n",
    "        \"\"\" Подсчет появлений слова в конкретном классе \"\"\"\n",
    "        #TODO частоты???\n",
    "        \n",
    "        counter = 0\n",
    "        masked_data = processed_data*(target==sms_class)\n",
    "        for sms in masked_data:\n",
    "            counter += sms.count(word)\n",
    "        return counter\n",
    "    \n",
    "\n",
    "    def fit(self, data: list, target: list):\n",
    "        \"\"\"\n",
    "        Обучение модели\n",
    "        \n",
    "        :param data: массив документов, каждый документ - объект типа str\n",
    "        :param target: массив меток объектов\n",
    "        :return: матрица Nx2 вероятностей P(x_i|c_j)\n",
    "        \"\"\"\n",
    "        #TODO частоты в словаре?\n",
    "        #TODO массив для условных вероятностей?\n",
    "        #TODO в каком порядке будут идти слова в матрице? Сортировка? \n",
    "        processed_data = [sms_preprocess(sms) for sms in data]\n",
    "        flat_data = [item for sublist in processed_data for item in sublist]\n",
    "        vocab_sorted = list(set(flat_data)).sorted()\n",
    "        \n",
    "        self.spam_num=(target==SPAM_CLASS).sum()\n",
    "        self.ham_num=(target==HAM_CLASS).sum()\n",
    "        \n",
    "        ham_freq = self.ham_num/len(data)\n",
    "        spam_freq = self.spam_num/len(data)\n",
    "        \n",
    "        self.vocabulary = {item:flat_data.count(item) for item in set(flat_data)}\n",
    "        \n",
    "        probability_matrix = [[words_num(processed_data, target, word, self.SPAM_CLASS)/spam_num,\n",
    "                               words_num(processed_data, target, word, self.HAM_CLASS)/ham_num] for word in vocab_sorted]\n",
    "        \n",
    "        \n",
    "        print('Fitted!')\n",
    "        return probability_matrix\n",
    "\n",
    "\n",
    "    def predict(self, data: list):\n",
    "        \"\"\"\n",
    "        :param data: массив документов, для каждого из которых нужно предсказать метку\n",
    "        :return: массив предсказанных меток\n",
    "        \"\"\"    \n",
    "        #TODO - поиск в словаре для предикта\n",
    "        predicted = []\n",
    "        processed_data = [text_preprocess(sms).split() for sms in data]\n",
    "        for sms in processed_data:\n",
    "            p_ham = 0\n",
    "            p_spam = 0\n",
    "            for word in sms:\n",
    "                temp_p_ham =0\n",
    "                temp_p_spam = 0\n",
    "                \n",
    "                \n",
    "                for entry in fitted:\n",
    "                    temp_p_ham+=entry[0]\n",
    "                    temp_p_spam+=entry[1]\n",
    "                \n",
    "                \n",
    "                p_ham+=temp_p_ham\n",
    "                p_spam+=temp_p_spam\n",
    "                \n",
    "            p_ham = p_ham*self.ham_freq\n",
    "            p_spam = p_spam*self.spam_freq\n",
    "            \n",
    "            if p_spam>p_ham:\n",
    "                predicted.append(SPAM_CLASS)\n",
    "            else:\n",
    "                predicted.append(HAM_CLASS)\n",
    "        \n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# прогон по нашему датасету\n",
    "\n",
    "classifier = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All fitted!\n",
      "Wall time: 6.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted = classifier.fit(df['sms'].values[:300],df['class'].values[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted = classifier.predict(df['sms'].values[300:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['class'].values[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = [True if predicted[i]==target[i] else False for i in range(len(predicted)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8704028021015762"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison.count(True)/len(comparison) # лол, 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sms_num = (df['class'] == SPAM_CLASS).sum()\n",
    "ham_sms_num = (df['class'] == HAM_CLASS).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8659368269921034"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_sms_num/(spam_sms_num + ham_sms_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hopes': 1,\n",
       " 'with': 25,\n",
       " 'dont': 15,\n",
       " 'available': 2,\n",
       " 'repair': 1,\n",
       " 'us': 4,\n",
       " 'friends': 6,\n",
       " 'g': 1,\n",
       " 'i‘m': 2,\n",
       " 'bus': 2,\n",
       " 'offered': 1,\n",
       " '145': 1,\n",
       " '16': 3,\n",
       " 'almost': 1,\n",
       " 'side': 1,\n",
       " 'club': 2,\n",
       " 'know': 13,\n",
       " 'haf': 2,\n",
       " 'starwars3': 1,\n",
       " '2wks': 1,\n",
       " 'ubandu': 1,\n",
       " 'suggest': 1,\n",
       " 'crave': 1,\n",
       " 'child': 1,\n",
       " 'wake': 1,\n",
       " 'kallis': 1,\n",
       " 'down': 6,\n",
       " '447801259231': 1,\n",
       " 'kanoil': 1,\n",
       " 'buying': 1,\n",
       " 'sign': 1,\n",
       " 'exam': 3,\n",
       " 'okie': 1,\n",
       " 'liked': 1,\n",
       " 'usual': 1,\n",
       " 'last': 3,\n",
       " 'wanted': 1,\n",
       " 'pleassssssseeeeee': 1,\n",
       " 'partner': 1,\n",
       " 'chores': 1,\n",
       " 'get': 16,\n",
       " '9am11pm': 1,\n",
       " 'however': 1,\n",
       " 'wwwareyouuniquecouk': 1,\n",
       " 'tncs': 1,\n",
       " 'watching': 2,\n",
       " 'important': 1,\n",
       " 'housework': 1,\n",
       " 'lucky': 1,\n",
       " 'already': 8,\n",
       " 'inviting': 1,\n",
       " 'requests': 1,\n",
       " 'no': 21,\n",
       " 'worried': 1,\n",
       " 'hours': 1,\n",
       " 'fa': 2,\n",
       " 'youll': 1,\n",
       " 'theory': 1,\n",
       " 'well': 6,\n",
       " 'we': 13,\n",
       " 'mmmmmm': 1,\n",
       " 'eg': 2,\n",
       " 'bday': 2,\n",
       " 'crazy': 1,\n",
       " 'dogg': 1,\n",
       " 'code': 5,\n",
       " 'incorrect': 1,\n",
       " 'needed': 1,\n",
       " 'pray': 1,\n",
       " 'dats': 1,\n",
       " 'module': 1,\n",
       " 'later': 6,\n",
       " 'q': 1,\n",
       " 'four': 1,\n",
       " '021': 1,\n",
       " 'ec2a': 1,\n",
       " 'bf': 1,\n",
       " 'stubborn': 1,\n",
       " '0845': 1,\n",
       " 'fallen': 1,\n",
       " 'eat': 1,\n",
       " 'c': 4,\n",
       " 'turn': 2,\n",
       " 'multis': 1,\n",
       " 'winner': 3,\n",
       " '12hrs': 2,\n",
       " 'kettoda': 1,\n",
       " 'mk45': 1,\n",
       " 'are': 36,\n",
       " 'knows': 1,\n",
       " 'satisfied': 1,\n",
       " 'forget': 1,\n",
       " '09064012160': 1,\n",
       " 'comin': 1,\n",
       " 'lemme': 1,\n",
       " 'ams': 1,\n",
       " 'car': 5,\n",
       " 'shracomorsglsuplt10': 1,\n",
       " 'vettam': 3,\n",
       " 'pleasure': 1,\n",
       " 'please': 9,\n",
       " 'evo': 1,\n",
       " 'dirt': 1,\n",
       " 'much': 5,\n",
       " 'openin': 1,\n",
       " 'that\\x92s': 2,\n",
       " 'words': 1,\n",
       " 'ansr': 1,\n",
       " 'del': 1,\n",
       " 'search': 1,\n",
       " 'got': 13,\n",
       " 'prepaid': 1,\n",
       " 'vaguely': 1,\n",
       " 'once': 2,\n",
       " 'actin': 1,\n",
       " 'dnt': 1,\n",
       " 'now': 29,\n",
       " 'sunny': 1,\n",
       " 'whole': 2,\n",
       " 'infowww100percentrealcom': 1,\n",
       " 'gt': 1,\n",
       " 'kr': 1,\n",
       " 'date': 2,\n",
       " 'liao': 2,\n",
       " 'apologetic': 1,\n",
       " 'wat': 2,\n",
       " 'callertune': 6,\n",
       " 'net': 2,\n",
       " 'think': 5,\n",
       " 'romantic': 1,\n",
       " 'have': 43,\n",
       " 'soon': 2,\n",
       " 'won': 6,\n",
       " 'pls': 6,\n",
       " 'horo': 2,\n",
       " 'wishes': 1,\n",
       " 'closer': 1,\n",
       " 'sao': 1,\n",
       " 'message': 4,\n",
       " '8am': 1,\n",
       " '0871277810810': 1,\n",
       " 'hot': 1,\n",
       " 'involve': 1,\n",
       " 'shining': 1,\n",
       " 'mall': 1,\n",
       " 'cruisin': 1,\n",
       " 'part': 2,\n",
       " 'verify': 2,\n",
       " 'jacket': 1,\n",
       " 'spent': 1,\n",
       " 'treat': 3,\n",
       " 'club4mobilescom': 1,\n",
       " 'smoke': 1,\n",
       " 'face': 1,\n",
       " 'expires': 1,\n",
       " 'questionstd': 1,\n",
       " 'birla': 1,\n",
       " 'coming': 3,\n",
       " 'fine': 2,\n",
       " 'shower': 1,\n",
       " 'boston': 2,\n",
       " 'nah': 1,\n",
       " 'lazy': 2,\n",
       " 'bath': 1,\n",
       " 'sometimes': 1,\n",
       " 'packing': 1,\n",
       " 'would': 3,\n",
       " 'ltgt': 11,\n",
       " 'gave': 2,\n",
       " 'receive': 5,\n",
       " 'mini': 1,\n",
       " 'didnt': 6,\n",
       " 'room': 3,\n",
       " 'subscription': 1,\n",
       " 'tonight': 5,\n",
       " 'formal': 1,\n",
       " 'txting': 1,\n",
       " 'wa': 1,\n",
       " 'xx': 2,\n",
       " 'dinnermsg': 1,\n",
       " 'nights': 1,\n",
       " 'walk': 1,\n",
       " '87077': 2,\n",
       " '£150': 2,\n",
       " 'wonderful': 1,\n",
       " 'yesterday': 1,\n",
       " 'wings': 1,\n",
       " 'cinema': 1,\n",
       " 'msn': 1,\n",
       " 'delivery': 3,\n",
       " 'off': 1,\n",
       " 'do': 17,\n",
       " 'link': 1,\n",
       " 'remember': 3,\n",
       " 'lunch': 6,\n",
       " 'finding': 1,\n",
       " 'lets': 1,\n",
       " 'innings': 1,\n",
       " 'holiday': 2,\n",
       " 'ends': 1,\n",
       " 'staying': 1,\n",
       " 'manda': 1,\n",
       " 'join': 2,\n",
       " 'operate': 1,\n",
       " 'lover': 1,\n",
       " 'services': 1,\n",
       " 'bother': 1,\n",
       " 'noun': 1,\n",
       " 'goalsteam': 1,\n",
       " 'loyalty': 1,\n",
       " 'contact': 6,\n",
       " 'trouble': 1,\n",
       " 'id': 2,\n",
       " 'mix': 1,\n",
       " 'big': 1,\n",
       " 'kkhow': 1,\n",
       " 'smth': 1,\n",
       " 'guys': 2,\n",
       " 'wkly': 3,\n",
       " 'past': 1,\n",
       " 'streetshall': 1,\n",
       " '186£150morefrmmob': 1,\n",
       " 'your': 42,\n",
       " 'female': 1,\n",
       " 'sooner': 1,\n",
       " '3aj': 1,\n",
       " 'arabian': 1,\n",
       " '08002986030': 1,\n",
       " 'someone': 5,\n",
       " 'dogging': 2,\n",
       " 'over': 2,\n",
       " 'ready': 4,\n",
       " 'wont': 4,\n",
       " 'lucyxx': 1,\n",
       " 'watch': 1,\n",
       " 'tmorrowpls': 1,\n",
       " 'open': 2,\n",
       " 'though': 3,\n",
       " 'mark': 2,\n",
       " 'who': 5,\n",
       " '69888': 1,\n",
       " 'sarcastic': 1,\n",
       " 'rates': 1,\n",
       " 'usf': 2,\n",
       " 'sorry': 11,\n",
       " 'price': 1,\n",
       " 'qatar': 1,\n",
       " 'sum1': 1,\n",
       " 'able': 1,\n",
       " 'if': 24,\n",
       " 'award': 1,\n",
       " 'aft': 1,\n",
       " 'balance': 1,\n",
       " 'ls1': 1,\n",
       " 'phone': 2,\n",
       " '6days': 1,\n",
       " 'using': 1,\n",
       " 'hell': 1,\n",
       " 'ya': 2,\n",
       " 'at': 20,\n",
       " 'correct': 2,\n",
       " 'cut': 3,\n",
       " 'private': 1,\n",
       " 'jus': 2,\n",
       " 'gravel': 1,\n",
       " 'fyi': 1,\n",
       " 'etc': 1,\n",
       " 'diskyou': 1,\n",
       " '07732584351': 1,\n",
       " 'gent': 1,\n",
       " 'touch': 1,\n",
       " '09064019788': 1,\n",
       " 'evening': 1,\n",
       " 'year': 2,\n",
       " 'she': 8,\n",
       " 'settled': 1,\n",
       " 'voda': 1,\n",
       " 'algarve': 1,\n",
       " 'seeing': 1,\n",
       " 'stuff': 4,\n",
       " 'entitled': 1,\n",
       " 'aids': 1,\n",
       " 'rreveal': 1,\n",
       " 'joking': 2,\n",
       " 'appropriate': 1,\n",
       " 'double': 2,\n",
       " 'earn': 1,\n",
       " 'stuff42moro': 1,\n",
       " '62468': 1,\n",
       " 'go': 16,\n",
       " 'does': 5,\n",
       " 'meant': 1,\n",
       " 'blessing': 1,\n",
       " 'spoke': 1,\n",
       " 'finally': 2,\n",
       " 'floor': 1,\n",
       " 'r': 5,\n",
       " 'grumpy': 1,\n",
       " 'will': 20,\n",
       " 'missed': 1,\n",
       " 'making': 1,\n",
       " 'click': 2,\n",
       " 'passed': 1,\n",
       " 'goto': 1,\n",
       " 'prediction': 1,\n",
       " 'offer': 2,\n",
       " 'wen': 4,\n",
       " 'age': 2,\n",
       " '2': 22,\n",
       " 'claim': 11,\n",
       " 'sucker': 1,\n",
       " 'although': 1,\n",
       " 'until': 4,\n",
       " 'kens': 1,\n",
       " 'congrats': 1,\n",
       " 'edukkukayee': 1,\n",
       " 'by': 7,\n",
       " 'unredeemed': 1,\n",
       " 'how': 11,\n",
       " 'specially': 2,\n",
       " 'ratetcs': 1,\n",
       " 'sick': 1,\n",
       " 'star': 1,\n",
       " 'review': 1,\n",
       " 'xxx': 4,\n",
       " 'take': 5,\n",
       " 'true': 1,\n",
       " 'xxxmobilemovieclub': 1,\n",
       " 'years': 1,\n",
       " 'callsmessagesmissed': 1,\n",
       " 'yr': 1,\n",
       " 'signing': 1,\n",
       " '150pwk': 1,\n",
       " 'research': 1,\n",
       " 'bugis': 1,\n",
       " 'gram': 2,\n",
       " 'accomodations': 1,\n",
       " 'umma': 2,\n",
       " 'him': 4,\n",
       " 'suckers': 1,\n",
       " 'avent': 1,\n",
       " 'can': 17,\n",
       " 'has': 8,\n",
       " 'lying': 1,\n",
       " 'pain': 4,\n",
       " 'deal': 1,\n",
       " 'ice': 1,\n",
       " 'died': 1,\n",
       " 'ge': 1,\n",
       " 'fone': 2,\n",
       " 'collect': 1,\n",
       " 'kkgoodstudy': 1,\n",
       " 'good': 7,\n",
       " 'points': 2,\n",
       " 'everyone': 1,\n",
       " 'lect': 1,\n",
       " 'page': 1,\n",
       " 'maybe': 1,\n",
       " 'august': 1,\n",
       " 'tb': 1,\n",
       " '89545': 1,\n",
       " 'poboxox36504w45wq': 1,\n",
       " 'were': 5,\n",
       " 'tel': 1,\n",
       " '£900': 2,\n",
       " 'every': 1,\n",
       " 'considering': 1,\n",
       " 'juz': 1,\n",
       " 'svc': 1,\n",
       " 'cheers': 1,\n",
       " 'copy': 4,\n",
       " 'tomarrow': 1,\n",
       " '2004': 2,\n",
       " 'doing': 5,\n",
       " 'soft': 1,\n",
       " 'than': 1,\n",
       " 'gandhipuram': 1,\n",
       " 'live': 4,\n",
       " 'numbers': 1,\n",
       " 'mrt': 1,\n",
       " 'fulfil': 1,\n",
       " 'puttin': 1,\n",
       " 'class': 4,\n",
       " 'both': 1,\n",
       " 'wine': 1,\n",
       " 'question': 1,\n",
       " 'casualty': 1,\n",
       " 'roommates': 2,\n",
       " '£100000': 1,\n",
       " 'shoot': 1,\n",
       " 'camcorder': 2,\n",
       " 'hear': 2,\n",
       " 'direct': 1,\n",
       " 'friend': 3,\n",
       " 'mu': 2,\n",
       " 'goodfine': 1,\n",
       " 'club4': 1,\n",
       " 'exist': 1,\n",
       " 'sheets': 1,\n",
       " 'persons': 2,\n",
       " 'xuhui': 1,\n",
       " 'yes': 8,\n",
       " 'not': 28,\n",
       " 'luv': 2,\n",
       " 'set': 3,\n",
       " 'receivea': 1,\n",
       " 'app': 1,\n",
       " 'decided': 1,\n",
       " 'ask': 1,\n",
       " 'rply': 1,\n",
       " 'handsome': 1,\n",
       " 'country': 1,\n",
       " 'nt': 1,\n",
       " 'lots': 1,\n",
       " 'drpd': 1,\n",
       " 'shop': 1,\n",
       " 'coins': 1,\n",
       " 'collected': 1,\n",
       " 'password': 1,\n",
       " 'ranjith': 1,\n",
       " 'bt': 1,\n",
       " 'next': 3,\n",
       " 'ur': 23,\n",
       " 'nipost': 1,\n",
       " 'letter': 1,\n",
       " 'a': 85,\n",
       " 'wid': 1,\n",
       " 'asked': 2,\n",
       " 'back': 10,\n",
       " 'bangbabes': 1,\n",
       " '07046744435': 1,\n",
       " 'surname': 1,\n",
       " 'valued': 2,\n",
       " 'usher': 1,\n",
       " '3days': 1,\n",
       " 'sleeping': 1,\n",
       " '82277': 1,\n",
       " 'oru': 3,\n",
       " 'because': 4,\n",
       " 'maneesha': 1,\n",
       " 'die': 1,\n",
       " 'yours': 2,\n",
       " 'eighth': 1,\n",
       " 'sunday': 1,\n",
       " 'plane': 1,\n",
       " 'sent': 5,\n",
       " 'here': 8,\n",
       " 'jackpot': 1,\n",
       " 'ga': 1,\n",
       " 'england': 2,\n",
       " 'secret': 1,\n",
       " 'sure': 6,\n",
       " 'ltdecimalgt': 3,\n",
       " 'late': 2,\n",
       " 'k': 7,\n",
       " 'really': 5,\n",
       " 'machan': 1,\n",
       " 'chances': 1,\n",
       " 'watts': 1,\n",
       " 'content': 2,\n",
       " 'genuine': 1,\n",
       " 'charge': 1,\n",
       " 'hockey': 1,\n",
       " 'worry': 1,\n",
       " 'fun': 1,\n",
       " 'pass': 1,\n",
       " 'nitros': 1,\n",
       " 'school': 2,\n",
       " 'mom': 1,\n",
       " 'successfully': 1,\n",
       " 'gautham': 1,\n",
       " 'guess': 4,\n",
       " 'text': 13,\n",
       " 'my': 56,\n",
       " 'situation': 4,\n",
       " 'steed': 1,\n",
       " 'knowyetunde': 1,\n",
       " 'vava': 1,\n",
       " 'between': 2,\n",
       " 'meeting': 1,\n",
       " 'working': 2,\n",
       " 'pick': 3,\n",
       " 'malarky': 1,\n",
       " 'unsubscribe': 1,\n",
       " 'guaranteed': 5,\n",
       " 'credit': 1,\n",
       " 'buffet': 1,\n",
       " 'wheres': 1,\n",
       " 'break': 1,\n",
       " 'sehwag': 1,\n",
       " 'sorting': 1,\n",
       " 'saturday': 1,\n",
       " 'interview': 1,\n",
       " 'cd': 1,\n",
       " 'tho': 2,\n",
       " 'men': 1,\n",
       " 'prize': 8,\n",
       " 'ill': 12,\n",
       " 'seekers': 1,\n",
       " 'weekends': 2,\n",
       " 'apply': 3,\n",
       " 'slippers': 1,\n",
       " 'rodger': 1,\n",
       " 'reward': 1,\n",
       " 'todayfrom': 1,\n",
       " 'after': 5,\n",
       " 'enc': 1,\n",
       " 'bit': 6,\n",
       " 'invite': 1,\n",
       " 'youd': 1,\n",
       " 'comp': 1,\n",
       " 'far': 1,\n",
       " 'lot': 3,\n",
       " 'paying': 1,\n",
       " 'chgs': 1,\n",
       " 'tea': 1,\n",
       " 'lives': 1,\n",
       " 'simply': 1,\n",
       " 'shipping': 1,\n",
       " 'barbie': 1,\n",
       " 'factory': 1,\n",
       " 'bank': 2,\n",
       " 'nver': 1,\n",
       " 'bcums': 1,\n",
       " 'urgnt': 1,\n",
       " 'britney': 1,\n",
       " 'conducts': 2,\n",
       " 'headin': 1,\n",
       " 'end': 6,\n",
       " '786': 1,\n",
       " 'alright': 2,\n",
       " 'frnds': 1,\n",
       " 'hard': 2,\n",
       " 'ahold': 1,\n",
       " 'hi': 9,\n",
       " 'breather': 1,\n",
       " 'valuable': 1,\n",
       " 'world': 4,\n",
       " 'mob': 2,\n",
       " 'mobiles': 2,\n",
       " 'representative': 2,\n",
       " 'server': 1,\n",
       " 'goodnight': 1,\n",
       " 'frying': 1,\n",
       " 'give': 8,\n",
       " 'til': 1,\n",
       " 'ring': 1,\n",
       " 'quality': 1,\n",
       " 'grave': 1,\n",
       " '3': 3,\n",
       " '£5month': 1,\n",
       " 'hols': 1,\n",
       " 'sportsx': 1,\n",
       " 'beloved': 1,\n",
       " 'orchard': 1,\n",
       " 'mummy': 1,\n",
       " 'doesnt': 2,\n",
       " 'wana': 1,\n",
       " 'place': 6,\n",
       " 'cabin': 2,\n",
       " 'tool': 1,\n",
       " 'turns': 1,\n",
       " 'pounds': 2,\n",
       " 'thanx': 2,\n",
       " 'wwwdbuknet': 1,\n",
       " 'worries': 1,\n",
       " 'sounds': 2,\n",
       " '07742676969': 1,\n",
       " 'nokia': 2,\n",
       " 'girls': 1,\n",
       " 'anymore': 2,\n",
       " '£5000': 2,\n",
       " 'card': 1,\n",
       " 'detroit': 1,\n",
       " 'cultures': 1,\n",
       " 'ahhh': 1,\n",
       " 'news': 1,\n",
       " 'appointment': 1,\n",
       " 'scotland': 1,\n",
       " 'k52': 1,\n",
       " 'jokes': 1,\n",
       " 'freephone': 1,\n",
       " 'moviewat': 1,\n",
       " 'keepintouch': 1,\n",
       " 'safe': 1,\n",
       " 'too': 8,\n",
       " 'bx420ip45we': 1,\n",
       " '2nd': 2,\n",
       " 'rubber': 1,\n",
       " 'xmas': 1,\n",
       " 'rcv': 2,\n",
       " 'which': 6,\n",
       " 'locations': 1,\n",
       " 'must': 4,\n",
       " 'very': 5,\n",
       " 'they': 2,\n",
       " 'wwwsmsacubootydelious': 1,\n",
       " 'account': 4,\n",
       " 'wk': 1,\n",
       " 'had': 4,\n",
       " 'sentence': 1,\n",
       " 'becoz': 1,\n",
       " 'check': 4,\n",
       " 'teaches': 2,\n",
       " 'music': 1,\n",
       " '0808': 1,\n",
       " 'personal': 1,\n",
       " '08712300220': 1,\n",
       " 'am': 13,\n",
       " 'anyways': 1,\n",
       " 'loses': 1,\n",
       " 'house': 2,\n",
       " 'dvd': 1,\n",
       " 'bribe': 1,\n",
       " 'divorce': 1,\n",
       " '08719180248': 1,\n",
       " 'respect': 2,\n",
       " '25': 2,\n",
       " 'girl': 2,\n",
       " 'today': 8,\n",
       " 'hmmm': 1,\n",
       " 'unlimited': 1,\n",
       " 'celebrations': 1,\n",
       " 'pa': 3,\n",
       " 'uks': 1,\n",
       " 'location': 1,\n",
       " 'new': 10,\n",
       " 'forever': 1,\n",
       " 'calls': 2,\n",
       " 'wait': 1,\n",
       " 'freemsg': 2,\n",
       " 'where': 6,\n",
       " 'finishes': 1,\n",
       " 'told': 2,\n",
       " 'feel': 5,\n",
       " 'kim': 1,\n",
       " 'wil': 1,\n",
       " 'jealous': 1,\n",
       " 'abt': 2,\n",
       " 'hep': 1,\n",
       " 'isnt': 1,\n",
       " 'msgs': 1,\n",
       " 'whats': 2,\n",
       " 'wins': 1,\n",
       " 'song': 2,\n",
       " 'ah': 2,\n",
       " '87121': 2,\n",
       " 'happen': 1,\n",
       " 'fml': 1,\n",
       " 'ac': 1,\n",
       " 'something': 3,\n",
       " 'hows': 1,\n",
       " '69988': 1,\n",
       " 'babyjontet': 1,\n",
       " 'or': 22,\n",
       " 'started': 1,\n",
       " 'objection': 1,\n",
       " 'needs': 1,\n",
       " 'real': 4,\n",
       " 'loans': 1,\n",
       " 'naked': 1,\n",
       " 'nurungu': 3,\n",
       " 'bslvyl': 1,\n",
       " 'b': 3,\n",
       " 'u': 57,\n",
       " 'uncle': 1,\n",
       " 'escape': 1,\n",
       " 'todays': 1,\n",
       " 'decide': 1,\n",
       " 'shirt': 1,\n",
       " 'windows': 1,\n",
       " 'lessons': 1,\n",
       " '125gift': 1,\n",
       " 'randy': 1,\n",
       " 'tickets': 1,\n",
       " 'usually': 1,\n",
       " 'semester': 1,\n",
       " '09061701461': 1,\n",
       " 'thk': 3,\n",
       " 'embarassed': 1,\n",
       " 'he': 11,\n",
       " 'use': 3,\n",
       " 'tonitebusy': 1,\n",
       " 'finished': 2,\n",
       " 's89': 1,\n",
       " 'stand': 1,\n",
       " 'towards': 2,\n",
       " 'point': 1,\n",
       " 'ahsen': 1,\n",
       " 'acoentry41': 1,\n",
       " '32f': 1,\n",
       " '5min': 1,\n",
       " 'bed': 1,\n",
       " 'x': 3,\n",
       " 'driving': 1,\n",
       " 'being': 4,\n",
       " '£2000': 1,\n",
       " 'long': 2,\n",
       " 'sending': 1,\n",
       " 'naughty': 1,\n",
       " 'mail': 2,\n",
       " 'cal': 1,\n",
       " 'blue': 2,\n",
       " '5903': 1,\n",
       " 'trust': 1,\n",
       " 'rs': 1,\n",
       " 'dearly': 1,\n",
       " 'sis': 2,\n",
       " 'oh': 3,\n",
       " 'nigeria': 1,\n",
       " 'completed': 1,\n",
       " 'mrng': 1,\n",
       " 'occupy': 1,\n",
       " 'www4tcbiz': 1,\n",
       " 'dresser': 1,\n",
       " 'cant': 5,\n",
       " 'plaza': 1,\n",
       " 'meare': 1,\n",
       " 'bloody': 2,\n",
       " 'latest': 1,\n",
       " 'tomorrow': 3,\n",
       " 'eatin': 1,\n",
       " 'looking': 4,\n",
       " 'carlos': 1,\n",
       " 'thank': 1,\n",
       " 'things': 4,\n",
       " 'speak': 4,\n",
       " 'learn': 1,\n",
       " '150ppm': 2,\n",
       " 'dream': 2,\n",
       " 'reach': 1,\n",
       " 'ffffffffff': 1,\n",
       " 'gonna': 3,\n",
       " 'anythin': 1,\n",
       " 'credits': 1,\n",
       " 'bak': 1,\n",
       " 'tmobile': 1,\n",
       " 'entered': 2,\n",
       " 'hello': 4,\n",
       " 'lovable': 2,\n",
       " '150pm': 2,\n",
       " 'final': 3,\n",
       " 'bat': 1,\n",
       " 'six': 1,\n",
       " 'cuz': 1,\n",
       " '500': 3,\n",
       " 'hes': 3,\n",
       " 'food': 2,\n",
       " 'xxxmobilemovieclubcomnqjkgighjjgcbl': 1,\n",
       " 'eggpotato': 1,\n",
       " 'wun': 1,\n",
       " 'up': 13,\n",
       " 'gf': 1,\n",
       " 'spanish': 1,\n",
       " 'national': 1,\n",
       " 'recdthirtyeight': 1,\n",
       " 'close': 2,\n",
       " 'word': 2,\n",
       " 'been': 12,\n",
       " 'installing': 1,\n",
       " 'lol': 4,\n",
       " 'cam': 1,\n",
       " 'inches': 1,\n",
       " 'txt': 13,\n",
       " 'myself': 2,\n",
       " 'necessarily': 1,\n",
       " 'taking': 1,\n",
       " 'thinked': 1,\n",
       " 'hungry': 1,\n",
       " 'course': 2,\n",
       " 'each': 2,\n",
       " 'anything': 7,\n",
       " 'meet': 6,\n",
       " 'series': 1,\n",
       " 'accomodate': 1,\n",
       " 'ave': 1,\n",
       " 'predict': 1,\n",
       " 'elama': 1,\n",
       " 'minute': 1,\n",
       " 'statement': 1,\n",
       " 'envy': 1,\n",
       " 'rain': 2,\n",
       " 'box': 1,\n",
       " 'gud': 2,\n",
       " 'po': 3,\n",
       " 'coffee': 1,\n",
       " 'granted': 1,\n",
       " 'deeraj': 1,\n",
       " 'hospital': 1,\n",
       " 'worth': 1,\n",
       " 'üll': 1,\n",
       " 'awesome': 2,\n",
       " '4403ldnw1a7rw18': 1,\n",
       " 'need': 9,\n",
       " 'kick': 1,\n",
       " 'suprman': 1,\n",
       " '85069': 1,\n",
       " 'quiz': 1,\n",
       " 'parentsi': 1,\n",
       " 'lor': 4,\n",
       " 'to': 132,\n",
       " 'then': 12,\n",
       " 'yup': 5,\n",
       " 'home': 11,\n",
       " 'internetservice': 1,\n",
       " 'yummy': 2,\n",
       " 'right': 5,\n",
       " 'choose': 2,\n",
       " 'networks': 1,\n",
       " 'draw': 4,\n",
       " 'inour': 1,\n",
       " 'what': 10,\n",
       " 'help': 2,\n",
       " 'me': 45,\n",
       " 'convincing': 1,\n",
       " 'immunisation': 1,\n",
       " 'annoncement': 1,\n",
       " 'of': 32,\n",
       " 'toll': 1,\n",
       " 'commercial': 1,\n",
       " 'forced': 1,\n",
       " 'youve': 2,\n",
       " 'sptv': 2,\n",
       " '2005': 1,\n",
       " 'goodo': 1,\n",
       " 'ki': 1,\n",
       " 'chart': 1,\n",
       " 'following': 1,\n",
       " 'address': 3,\n",
       " 'sense': 1,\n",
       " 'since': 1,\n",
       " 'gentleman': 2,\n",
       " 'prob': 2,\n",
       " 'approaches': 1,\n",
       " 'jersey': 1,\n",
       " 'the': 79,\n",
       " 'bcoz': 1,\n",
       " 'hospitals': 1,\n",
       " 'dead': 1,\n",
       " 'battery': 1,\n",
       " '1pm': 1,\n",
       " 'theres': 2,\n",
       " 'try': 4,\n",
       " 'babe': 3,\n",
       " 'in': 49,\n",
       " 'as': 16,\n",
       " 'cave': 1,\n",
       " 'stool': 1,\n",
       " 'strict': 1,\n",
       " 'lturlgt': 1,\n",
       " 'staff': 1,\n",
       " 'league': 1,\n",
       " 'hamster': 1,\n",
       " 'crashing': 1,\n",
       " 'eating': 1,\n",
       " 'membership': 1,\n",
       " 'yeshere': 1,\n",
       " 'bootydelious': 1,\n",
       " 'caught': 1,\n",
       " 'says': 1,\n",
       " 'sees': 1,\n",
       " 'experience': 1,\n",
       " 'okay': 2,\n",
       " 'yes434': 1,\n",
       " 'forgot': 3,\n",
       " 'theater': 1,\n",
       " 'getting': 3,\n",
       " 'replied': 1,\n",
       " 'endowed': 1,\n",
       " 'took': 1,\n",
       " 'operator': 2,\n",
       " 'hee': 1,\n",
       " 'customer': 6,\n",
       " 'just': 20,\n",
       " 'mins': 1,\n",
       " 'sucks': 1,\n",
       " 'macedonia': 1,\n",
       " 'discuss': 1,\n",
       " 'pobox': 1,\n",
       " 'greatbye': 1,\n",
       " 'killing': 1,\n",
       " 'fair': 1,\n",
       " 'else': 1,\n",
       " 'brother': 1,\n",
       " 'files': 1,\n",
       " 'narcotics': 1,\n",
       " 'months': 2,\n",
       " '87066': 1,\n",
       " 'v': 11,\n",
       " 'cash': 6,\n",
       " 'hit': 1,\n",
       " 'top': 2,\n",
       " 'itself': 2,\n",
       " 'transaction': 1,\n",
       " 'around': 3,\n",
       " 'badly': 1,\n",
       " 'wit': 1,\n",
       " 'realized': 1,\n",
       " 'apologise': 1,\n",
       " 'slept': 1,\n",
       " 'doinghow': 1,\n",
       " 'entry': 3,\n",
       " 'msg': 5,\n",
       " 'press': 3,\n",
       " 'times': 3,\n",
       " '3680offer': 1,\n",
       " 'tc': 1,\n",
       " '09061209465': 1,\n",
       " 'straight': 1,\n",
       " 'ors': 1,\n",
       " 'run': 2,\n",
       " 'goodmorning': 1,\n",
       " 'arrange': 1,\n",
       " 'darling': 1,\n",
       " 'company': 2,\n",
       " 'week': 4,\n",
       " 'youhow': 1,\n",
       " 'half': 2,\n",
       " 'pours': 1,\n",
       " 'hmmmy': 1,\n",
       " 'goes': 1,\n",
       " 'complimentary': 1,\n",
       " 'boytoy': 1,\n",
       " '30th': 1,\n",
       " 'free': 19,\n",
       " 'chat': 3,\n",
       " 'tell': 5,\n",
       " 'info': 1,\n",
       " 'for': 45,\n",
       " 'e': 6,\n",
       " 'mr': 1,\n",
       " 'dot': 1,\n",
       " 'darlin': 1,\n",
       " 'largest': 1,\n",
       " 'hail': 1,\n",
       " 'birthday': 2,\n",
       " 'loud': 1,\n",
       " 'better': 3,\n",
       " 'called': 2,\n",
       " 'melle': 6,\n",
       " 'okvarunnathu': 1,\n",
       " 'awarded': 4,\n",
       " 'also': 2,\n",
       " 'sms': 3,\n",
       " 'an': 2,\n",
       " 'cozsomtimes': 1,\n",
       " '69698': 2,\n",
       " 'singles': 1,\n",
       " 'dignity': 2,\n",
       " 'burns': 1,\n",
       " 'usps': 1,\n",
       " 'match': 2,\n",
       " 'waiting': 4,\n",
       " 'start': 1,\n",
       " 'gr8': 2,\n",
       " 'alter': 1,\n",
       " 'anybody': 1,\n",
       " 'yesgauti': 1,\n",
       " '11': 2,\n",
       " 'hmv': 3,\n",
       " 'kkwhere': 1,\n",
       " 'fainting': 1,\n",
       " 'applespairsall': 1,\n",
       " 'sindu': 1,\n",
       " '750': 1,\n",
       " 'street': 2,\n",
       " 'jolt': 1,\n",
       " 'wet': 1,\n",
       " 'begin': 1,\n",
       " 'any': 3,\n",
       " 'box1146': 1,\n",
       " '0125698789': 1,\n",
       " '28thfebtcs': 1,\n",
       " '7548': 1,\n",
       " 'oso': 1,\n",
       " 'kate': 2,\n",
       " 'name': 6,\n",
       " 'is': 38,\n",
       " 'sptyrone': 1,\n",
       " 'tas': 1,\n",
       " 'smiling': 1,\n",
       " '4txtú120': 1,\n",
       " 'story': 1,\n",
       " 'hurt': 1,\n",
       " 'them': 5,\n",
       " 'happy': 6,\n",
       " 'plural': 1,\n",
       " 'askd': 2,\n",
       " 're': 2,\n",
       " 'cried': 1,\n",
       " 'signin': 1,\n",
       " '81010': 1,\n",
       " 'career': 1,\n",
       " 'day': 7,\n",
       " 'ride': 1,\n",
       " 'on': 37,\n",
       " 'buy': 4,\n",
       " 'there': 10,\n",
       " 'way': 11,\n",
       " 'changed': 3,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
